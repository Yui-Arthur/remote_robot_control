{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def expand_row_data(data, capture_point) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    label = []\n",
    "    expand_data = []\n",
    "\n",
    "    for idx in range(1, len(data) - capture_point*2, 6):\n",
    "        label.append(int(data[0]))\n",
    "        # \n",
    "        n_data = np.array([], dtype=np.float32)\n",
    "        for i in range(0, capture_point*2, 6):\n",
    "            # n_data = np.concatenate((n_data, data[idx + i + 3 : idx + i + 6]), axis=0, dtype=np.float32)\n",
    "            n_data = np.concatenate((n_data, data[idx + i + 0 : idx + i + 3]), axis=0, dtype=np.float32)\n",
    "            \n",
    "        expand_data.append(n_data)\n",
    "        # print(n_data.shape)\n",
    "        # raise ValueError\n",
    "        # \n",
    "        # expand_data.append(np.array(data[idx : idx + capture_point], dtype=np.float32))\n",
    "\n",
    "    return label, expand_data\n",
    "\n",
    "def gen_tf_dataset(data, capture_point, batch_size, output_class):\n",
    "    label = []\n",
    "    expanded_data = []\n",
    "    for row in data:\n",
    "        l, e = expand_row_data(row, capture_point)    \n",
    "        \n",
    "        label += l\n",
    "        expanded_data += e\n",
    "    label = tf.one_hot(label, output_class)  \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((expanded_data, label))\n",
    "    dataset = dataset.shuffle(len(data), reshuffle_each_iteration=True)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def dataset_preproccessed(data_folder, batch_size, train_vaild_test_ratio, output_class):\n",
    "    data_folder = Path(data_folder)\n",
    "\n",
    "    if len(data_folder.stem.split('_')) == 3:\n",
    "        capture_point, extra_point = data_folder.stem.split('_')[:-1]\n",
    "    else:\n",
    "        capture_point, extra_point, _ = data_folder.stem.split('_')[:-1]\n",
    "    # capture_point = int(capture_point[1:]) * 6\n",
    "    capture_point = int(capture_point[1:]) * 3\n",
    "    extra_point = int(extra_point[1:])\n",
    "\n",
    "    \n",
    "    with open(data_folder, newline='') as data:\n",
    "        row_data = csv.reader(data, delimiter=',')\n",
    "        row_data = [i for i in row_data]\n",
    "\n",
    "    random.shuffle(row_data)\n",
    "    \n",
    "    train_data_cnt = int(len(row_data) * train_vaild_test_ratio[0])\n",
    "    valid_data_cnt = int(len(row_data) * train_vaild_test_ratio[1])\n",
    "    test_data_cnt = len(row_data) - train_data_cnt - valid_data_cnt\n",
    "    print(capture_point, extra_point)\n",
    "    print(train_data_cnt, valid_data_cnt, test_data_cnt)\n",
    "    \n",
    "    for idx, row in enumerate(row_data):\n",
    "        for c_idx, _ in enumerate(row): \n",
    "\n",
    "            row_data[idx][c_idx] = float(row[c_idx])\n",
    "\n",
    "            if c_idx == 0: continue\n",
    "\n",
    "            # if (c_idx-1) % 6 < 3 : row_data[idx][c_idx] = (row_data[idx][c_idx] + 4.0) / 8.0\n",
    "            if (c_idx-1) % 6 < 3 : row_data[idx][c_idx] = row_data[idx][c_idx]\n",
    "            else: row_data[idx][c_idx] = (row_data[idx][c_idx] + 2000.0) / 4000.0\n",
    "\n",
    "    train_dataset = gen_tf_dataset(row_data[:train_data_cnt], capture_point, batch_size, output_class)\n",
    "    valid_dataset = gen_tf_dataset(row_data[train_data_cnt : train_data_cnt + valid_data_cnt], capture_point, batch_size, output_class)\n",
    "    # test_dataset = gen_tf_dataset(row_data[train_data_cnt + valid_data_cnt : ], capture_point, batch_size, output_class)\n",
    "    test_dataset = None\n",
    "    print(len(train_dataset) * batch_size)\n",
    "    print(len(valid_dataset) * batch_size)\n",
    "    # print(len(test_dataset) * batch_size)\n",
    "    \n",
    "    return capture_point, train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pseudo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pseudo_dataset(size, capture_point, batch_size, output_class):\n",
    "    label = []\n",
    "    expanded_data = []\n",
    "\n",
    "    x_values = np.random.uniform(low=0, high= output_class * 1000, size=size).astype(np.float32)\n",
    "\n",
    "    for x in x_values:\n",
    "        l = int(x // 1000) \n",
    "\n",
    "        e = [l for i in range(capture_point)]\n",
    "\n",
    "        label += [l]\n",
    "        expanded_data += [e]\n",
    "    label = tf.one_hot(label, output_class)  \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((expanded_data, label))\n",
    "    dataset = dataset.shuffle(size, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def pseudo_dataset_preproccessed(total_data_size ,capture_point, batch_size, train_vaild_test_ratio, output_class):\n",
    "\n",
    "    \n",
    "    train_data_cnt = int(total_data_size * train_vaild_test_ratio[0])\n",
    "    valid_data_cnt = int(total_data_size * train_vaild_test_ratio[1])\n",
    "    test_data_cnt = total_data_size- train_data_cnt - valid_data_cnt\n",
    "    print(capture_point)\n",
    "    print(train_data_cnt, valid_data_cnt, test_data_cnt)\n",
    "\n",
    "    train_dataset = gen_pseudo_dataset(train_data_cnt, capture_point, batch_size, output_class)\n",
    "    valid_dataset = gen_pseudo_dataset(valid_data_cnt, capture_point, batch_size, output_class)\n",
    "    test_dataset = gen_pseudo_dataset(test_data_cnt, capture_point, batch_size, output_class)\n",
    "    \n",
    "    return capture_point, train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(tf.keras.Model):\n",
    "    def __init__(self , input_dim , out_class , learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.out_class = out_class\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self._model = self._build_model()\n",
    "        self._learner = self._build_learner()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x:tf.Tensor, training:bool=False) -> tf.Tensor:\n",
    "        output = self._model(x, training=training)\n",
    "        return output\n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, x:tf.Tensor, y:tf.Tensor):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.__call__(x, training=True)\n",
    "            classLoss = self._learner[\"get_loss\"](y, output)\n",
    "            review = tf.math.in_top_k(tf.math.argmax(y,axis=1), output, 1)\n",
    "            perf = tf.math.reduce_mean(tf.cast(review, dtype=\"float32\"))\n",
    "\n",
    "        cGradients = tape.gradient(classLoss, self._model.trainable_variables)\n",
    "        self._learner[\"optimize\"].apply_gradients(zip(cGradients, self._model.trainable_variables))\n",
    "        return perf, classLoss\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def validate(self, x:tf.Tensor, y:tf.Tensor) -> tf.Tensor:\n",
    "        output = self.__call__(x, training=False)\n",
    "        classLoss = self._learner[\"get_loss\"](y , output)\n",
    "        review = tf.math.in_top_k(tf.math.argmax(y,axis=1), output, 1)\n",
    "        perf = tf.math.reduce_mean(tf.cast(review, dtype=\"float32\"))\n",
    "        return perf , classLoss\n",
    "\n",
    "    def _build_model(self) -> tf.keras.Model:\n",
    "\n",
    "        input_tensor = tf.keras.Input(shape=self.input_dim)\n",
    "        feature_map = input_tensor\n",
    "        \n",
    "        d = 32\n",
    "        feature_map = tf.keras.layers.Dense(d, input_dim = self.input_dim, activation='relu')(feature_map)\n",
    "        feature_map = tf.keras.layers.Dropout(0.1)(feature_map)\n",
    "\n",
    "        feature_map = tf.keras.layers.Dense(d // 4, input_dim = d, activation='relu')(feature_map)\n",
    "        feature_map = tf.keras.layers.Dropout(0.1)(feature_map)\n",
    "\n",
    "        feature_map = tf.keras.layers.Dense(d // 8, input_dim = d // 4, activation='relu')(feature_map)\n",
    "        feature_map = tf.keras.layers.Dropout(0.1)(feature_map)\n",
    "        \n",
    "        output_tensor = tf.keras.layers.Dense(self.out_class, input_dim = d // 8, activation=tf.keras.activations.softmax)(feature_map)\n",
    "        \n",
    "\n",
    "        model = tf.keras.Model(input_tensor, output_tensor)\n",
    "        return model\n",
    "    \n",
    "    def _build_learner(self) -> dict:\n",
    "        # classLoss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        classLoss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        # classOptimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        classOptimizer = tf.keras.optimizers.SGD(learning_rate=self.learning_rate)\n",
    "        learner = {\"get_loss\": classLoss, \"optimize\": classOptimizer}\n",
    "\n",
    "        return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model , train_dataloader , valid_dataloader , max_acc , root_dir : Path):\n",
    "\n",
    "    epoch_train_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_loss = []\n",
    "    epoch_valid_acc = []\n",
    "\n",
    "    for inData, outData in train_dataloader:\n",
    "        acc , loss = model.train(inData, outData)\n",
    "        epoch_train_acc.append(acc)\n",
    "        epoch_train_loss.append(loss)\n",
    "        # print(loss)\n",
    "        # print(acc)\n",
    "    # raise ValueError\n",
    "        \n",
    "    for inData, outData in valid_dataloader:\n",
    "        acc , loss = model.validate(inData, outData)\n",
    "        epoch_valid_acc.append(acc)\n",
    "        epoch_valid_loss.append(loss)\n",
    "\n",
    "    epoch_train_acc_mean = tf.math.reduce_mean(epoch_train_acc) * 100\n",
    "    epoch_valid_acc_mean = tf.math.reduce_mean(epoch_valid_acc) * 100\n",
    "    epoch_train_loss_mean = tf.math.reduce_mean(epoch_train_loss)\n",
    "    epoch_valid_loss_mean = tf.math.reduce_mean(epoch_valid_loss)\n",
    "\n",
    "    print(f\"  Train Acc: {epoch_train_acc_mean:.2f}, Loss: {epoch_train_loss_mean:.2f}\")\n",
    "    print(f\"  Valid Acc: {epoch_valid_acc_mean:.2f}, Loss: {epoch_valid_loss_mean:.2f}\")\n",
    "\n",
    "    if(epoch_valid_acc_mean > max_acc):\n",
    "        print(\"save best model\")\n",
    "        model.save(root_dir /\"model\", include_optimizer=False)\n",
    "    \n",
    "    return epoch_train_acc_mean, epoch_train_loss_mean , epoch_valid_acc_mean , epoch_valid_loss_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(dataset_path, capture_point, output_class, root_dir : Path = None , model_path : Path = None):\n",
    "    if root_dir is not None :\n",
    "        model = tf.keras.models.load_model(root_dir / \"model\", compile=False)\n",
    "    elif model_path is not None:\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "    else:\n",
    "        raise AttributeError(\"Testing models root_dir and model_path are not given\")\n",
    "    \n",
    "    infer_cnt = 0\n",
    "    acc_cnt = 0\n",
    "\n",
    "    print(\"Testing Model\")\n",
    "    \n",
    "    with open(dataset_path, newline='') as data:\n",
    "        row_data = csv.reader(data, delimiter=',')\n",
    "        row_data = [i for i in row_data]\n",
    "    \n",
    "    for idx, row in enumerate(row_data):\n",
    "        for c_idx, _ in enumerate(row): \n",
    "            row_data[idx][c_idx] = float(row[c_idx])\n",
    "            \n",
    "            if c_idx == 0: continue\n",
    "            # if (c_idx-1) % 6 < 3 : row_data[idx][c_idx] = (row_data[idx][c_idx] + 4.0) / 8.0\n",
    "            if (c_idx-1) % 6 < 3 : row_data[idx][c_idx] = row_data[idx][c_idx]\n",
    "            else: row_data[idx][c_idx] = (row_data[idx][c_idx] + 2000.0) / 4000.0\n",
    "            # else: row_data[idx][c_idx] = 0.0\n",
    "\n",
    "\n",
    "    for data in row_data:\n",
    "        pred_cnt = [0 for i in range(output_class)]\n",
    "        label = int(data[0])\n",
    "        for idx in range(1, len(data) - capture_point*2, 6):\n",
    "            input = np.array([], np.float32)\n",
    "            for i in range(0, capture_point*2, 6):\n",
    "                input = np.concatenate((input, data[idx + i + 0 : idx + i + 3]), axis=0, dtype=np.float32)\n",
    "            # input = np.array(data[idx : idx + capture_point], dtype=np.float32)\n",
    "            pred = model(np.expand_dims(input, axis=0))\n",
    "            pred_cnt[np.argmax(pred)] += 1\n",
    "        \n",
    "        final_pred = np.argmax(pred_cnt)\n",
    "        # print(pred_cnt, label, final_pred)\n",
    "        acc_cnt += (final_pred == label)\n",
    "        infer_cnt += 1\n",
    "        \n",
    "    print(f\"acc = {acc_cnt / infer_cnt}\")\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_results(train_info , root_folder : Path):\n",
    "    train_info = np.array(train_info)\n",
    "\n",
    "    plt.plot(np.arange(1,train_info.shape[0]+1) , train_info[:,0] , 'r' , label='train')\n",
    "    plt.plot(np.arange(1,train_info.shape[0]+1) , train_info[:,2] , 'b' , label='valid')\n",
    "    plt.title(\"Acc\")\n",
    "    plt.savefig(root_folder / \"model\" / \"Acc.png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(1,train_info.shape[0]+1) , train_info[:,1] , 'r' , label='train')\n",
    "    plt.plot(np.arange(1,train_info.shape[0]+1) , train_info[:,3] , 'b' , label='valid')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.savefig(root_folder / \"model\" / \"Loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_tflite_quant(save_model_folder : Path , dataset):\n",
    "\n",
    "    def representative_dataset():\n",
    "        idx = 0\n",
    "        for data , label in dataset:\n",
    "            if idx > 1000:\n",
    "                break\n",
    "            idx += 1\n",
    "            \n",
    "            yield [data]\n",
    "\n",
    "    # float\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(str(save_model_folder))\n",
    "    tflite_model = converter.convert()\n",
    "    open(save_model_folder / 'float_model.tflite', 'wb').write(tflite_model)\n",
    "    print(\"Successfully convert tflite float\")\n",
    "\n",
    "    # quant\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(str(save_model_folder))\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    tflite_quant_model = converter.convert()\n",
    "    open(save_model_folder / 'quant_model.tflite', 'wb').write(tflite_quant_model)\n",
    "    print(\"Successfully convert tflite quant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tflite_model(tflite_model_path, dataset_path, capture_point, output_class):\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "    input = interpreter.get_input_details()[0]\n",
    "    output = interpreter.get_output_details()[0]\n",
    "    input_scale, input_zero_point = input['quantization']\n",
    "    output_scale, output_zero_point = output[\"quantization\"]\n",
    "    \n",
    "    print(f\"testing with {tflite_model_path}\")\n",
    "\n",
    "    infer_cnt = 0\n",
    "    acc_cnt = 0\n",
    "    \n",
    "    with open(dataset_path, newline='') as data:\n",
    "        row_data = csv.reader(data, delimiter=',')\n",
    "        row_data = [i for i in row_data]\n",
    "    \n",
    "    for idx, row in enumerate(row_data):\n",
    "        for c_idx, _ in enumerate(row): \n",
    "            row_data[idx][c_idx] = float(row[c_idx])\n",
    "\n",
    "            if c_idx == 0: continue\n",
    "            # if (c_idx-1) % 6 < 3 : row_data[idx][c_idx] = (row_data[idx][c_idx] + 4.0) / 8.0\n",
    "            if (c_idx-1) % 6 < 3 : row_data[idx][c_idx] = row_data[idx][c_idx]\n",
    "            else: row_data[idx][c_idx] = (row_data[idx][c_idx] + 2000.0) / 4000.0\n",
    "\n",
    "    for data in row_data:\n",
    "        pred_cnt = [0 for i in range(output_class)]\n",
    "        label = int(data[0])\n",
    "        for idx in range(1, len(data) - capture_point*2, 6):\n",
    "            d = np.array([], np.float32)\n",
    "            for i in range(0, capture_point*2, 6):\n",
    "                d = np.concatenate((d, data[idx + i + 0 : idx + i + 3]), axis=0, dtype=np.float32)\n",
    "        \n",
    "            d = np.expand_dims(d, axis=0)\n",
    "            if input[\"dtype\"] == np.float32:\n",
    "                interpreter.set_tensor(input['index'], d)\n",
    "            else:\n",
    "                quant_data = d / input_scale + input_zero_point\n",
    "                \n",
    "                interpreter.set_tensor(input['index'], quant_data.astype(input[\"dtype\"] ))\n",
    "            \n",
    "            interpreter.invoke()\n",
    "\n",
    "            if output[\"dtype\"] == np.float32:\n",
    "                pred = interpreter.get_tensor(output['index'])\n",
    "            else:\n",
    "                dequant_pred = interpreter.get_tensor(output['index']).astype(np.float32)\n",
    "                pred = (dequant_pred - output_zero_point) * output_scale\n",
    "\n",
    "            pred_cnt[np.argmax(pred)] += 1\n",
    "            \n",
    "        final_pred = np.argmax(pred_cnt)\n",
    "        # print(infer_cnt, pred_cnt, label, final_pred)\n",
    "        acc_cnt += (final_pred == label)\n",
    "        infer_cnt += 1\n",
    "        \n",
    "    \n",
    "    print(f\"acc = {acc_cnt / infer_cnt} ({acc_cnt} / {infer_cnt})\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 10\n",
      "126 14 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 16:49:39.412983: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2520\n",
      "280\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 60)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                1952      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 35        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2287 (8.93 KB)\n",
      "Trainable params: 2287 (8.93 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "  Train Acc: 33.81, Loss: 1.72\n",
      "  Valid Acc: 30.36, Loss: 1.48\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 51.94, Loss: 1.39\n",
      "  Valid Acc: 66.43, Loss: 1.20\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 54.13, Loss: 1.16\n",
      "  Valid Acc: 69.29, Loss: 0.89\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 62.26, Loss: 0.97\n",
      "  Valid Acc: 72.50, Loss: 0.68\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 69.76, Loss: 0.86\n",
      "  Valid Acc: 69.64, Loss: 0.64\n",
      "  Train Acc: 73.73, Loss: 0.78\n",
      "  Valid Acc: 76.43, Loss: 0.54\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 74.44, Loss: 0.75\n",
      "  Valid Acc: 76.43, Loss: 0.50\n",
      "  Train Acc: 76.07, Loss: 0.71\n",
      "  Valid Acc: 76.79, Loss: 0.46\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 76.35, Loss: 0.68\n",
      "  Valid Acc: 78.57, Loss: 0.42\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 74.96, Loss: 0.69\n",
      "  Valid Acc: 76.43, Loss: 0.40\n",
      "  Train Acc: 76.15, Loss: 0.66\n",
      "  Valid Acc: 78.21, Loss: 0.36\n",
      "  Train Acc: 78.21, Loss: 0.63\n",
      "  Valid Acc: 77.86, Loss: 0.34\n",
      "  Train Acc: 80.00, Loss: 0.59\n",
      "  Valid Acc: 79.29, Loss: 0.32\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 78.53, Loss: 0.61\n",
      "  Valid Acc: 79.29, Loss: 0.32\n",
      "  Train Acc: 79.96, Loss: 0.56\n",
      "  Valid Acc: 97.14, Loss: 0.27\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 80.36, Loss: 0.55\n",
      "  Valid Acc: 99.64, Loss: 0.24\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 79.64, Loss: 0.56\n",
      "  Valid Acc: 98.93, Loss: 0.24\n",
      "  Train Acc: 78.81, Loss: 0.57\n",
      "  Valid Acc: 98.93, Loss: 0.24\n",
      "  Train Acc: 79.56, Loss: 0.55\n",
      "  Valid Acc: 98.93, Loss: 0.23\n",
      "  Train Acc: 80.44, Loss: 0.54\n",
      "  Valid Acc: 97.86, Loss: 0.25\n",
      "  Train Acc: 80.95, Loss: 0.52\n",
      "  Valid Acc: 99.29, Loss: 0.21\n",
      "  Train Acc: 81.90, Loss: 0.50\n",
      "  Valid Acc: 99.64, Loss: 0.19\n",
      "  Train Acc: 80.24, Loss: 0.53\n",
      "  Valid Acc: 98.21, Loss: 0.21\n",
      "  Train Acc: 82.78, Loss: 0.49\n",
      "  Valid Acc: 98.57, Loss: 0.19\n",
      "  Train Acc: 82.06, Loss: 0.50\n",
      "  Valid Acc: 98.57, Loss: 0.20\n",
      "  Train Acc: 81.94, Loss: 0.50\n",
      "  Valid Acc: 98.21, Loss: 0.19\n",
      "  Train Acc: 83.53, Loss: 0.45\n",
      "  Valid Acc: 99.64, Loss: 0.14\n",
      "  Train Acc: 83.13, Loss: 0.48\n",
      "  Valid Acc: 98.21, Loss: 0.17\n",
      "  Train Acc: 84.13, Loss: 0.45\n",
      "  Valid Acc: 99.64, Loss: 0.14\n",
      "  Train Acc: 85.67, Loss: 0.42\n",
      "  Valid Acc: 99.29, Loss: 0.13\n",
      "  Train Acc: 85.36, Loss: 0.43\n",
      "  Valid Acc: 99.64, Loss: 0.12\n",
      "  Train Acc: 85.20, Loss: 0.43\n",
      "  Valid Acc: 99.29, Loss: 0.13\n",
      "  Train Acc: 85.32, Loss: 0.43\n",
      "  Valid Acc: 99.64, Loss: 0.11\n",
      "  Train Acc: 84.72, Loss: 0.43\n",
      "  Valid Acc: 98.21, Loss: 0.15\n",
      "  Train Acc: 85.79, Loss: 0.42\n",
      "  Valid Acc: 99.64, Loss: 0.11\n",
      "  Train Acc: 85.04, Loss: 0.42\n",
      "  Valid Acc: 99.29, Loss: 0.10\n",
      "  Train Acc: 85.12, Loss: 0.42\n",
      "  Valid Acc: 99.64, Loss: 0.11\n",
      "  Train Acc: 85.40, Loss: 0.41\n",
      "  Valid Acc: 100.00, Loss: 0.08\n",
      "save best model\n",
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Acc: 86.23, Loss: 0.39\n",
      "  Valid Acc: 99.64, Loss: 0.08\n",
      "  Train Acc: 85.12, Loss: 0.41\n",
      "  Valid Acc: 99.64, Loss: 0.09\n",
      "  Train Acc: 83.97, Loss: 0.43\n",
      "  Valid Acc: 99.64, Loss: 0.10\n",
      "  Train Acc: 85.79, Loss: 0.39\n",
      "  Valid Acc: 100.00, Loss: 0.08\n",
      "  Train Acc: 86.55, Loss: 0.37\n",
      "  Valid Acc: 100.00, Loss: 0.07\n",
      "  Train Acc: 86.51, Loss: 0.38\n",
      "  Valid Acc: 100.00, Loss: 0.08\n",
      "  Train Acc: 86.31, Loss: 0.38\n",
      "  Valid Acc: 100.00, Loss: 0.08\n",
      "  Train Acc: 85.95, Loss: 0.39\n",
      "  Valid Acc: 100.00, Loss: 0.08\n",
      "  Train Acc: 87.34, Loss: 0.35\n",
      "  Valid Acc: 100.00, Loss: 0.07\n",
      "  Train Acc: 86.19, Loss: 0.38\n",
      "  Valid Acc: 100.00, Loss: 0.08\n",
      "  Train Acc: 85.36, Loss: 0.39\n",
      "  Valid Acc: 98.21, Loss: 0.11\n",
      "  Train Acc: 86.43, Loss: 0.37\n",
      "  Valid Acc: 96.79, Loss: 0.13\n",
      "  Train Acc: 84.76, Loss: 0.41\n",
      "  Valid Acc: 100.00, Loss: 0.07\n",
      "  Train Acc: 86.47, Loss: 0.37\n",
      "  Valid Acc: 99.64, Loss: 0.08\n",
      "  Train Acc: 87.06, Loss: 0.36\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 86.94, Loss: 0.37\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 86.63, Loss: 0.36\n",
      "  Valid Acc: 100.00, Loss: 0.06\n",
      "  Train Acc: 86.39, Loss: 0.37\n",
      "  Valid Acc: 100.00, Loss: 0.06\n",
      "  Train Acc: 86.03, Loss: 0.37\n",
      "  Valid Acc: 100.00, Loss: 0.07\n",
      "  Train Acc: 86.11, Loss: 0.37\n",
      "  Valid Acc: 99.64, Loss: 0.06\n",
      "  Train Acc: 85.87, Loss: 0.38\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 85.91, Loss: 0.37\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 86.59, Loss: 0.37\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 87.66, Loss: 0.35\n",
      "  Valid Acc: 100.00, Loss: 0.07\n",
      "  Train Acc: 86.75, Loss: 0.35\n",
      "  Valid Acc: 99.64, Loss: 0.06\n",
      "  Train Acc: 86.23, Loss: 0.37\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 86.67, Loss: 0.36\n",
      "  Valid Acc: 99.64, Loss: 0.06\n",
      "  Train Acc: 86.43, Loss: 0.36\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 86.83, Loss: 0.35\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 87.34, Loss: 0.33\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 86.94, Loss: 0.35\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 87.14, Loss: 0.33\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 86.79, Loss: 0.36\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 88.13, Loss: 0.33\n",
      "  Valid Acc: 100.00, Loss: 0.03\n",
      "  Train Acc: 86.43, Loss: 0.35\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 86.03, Loss: 0.36\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 87.06, Loss: 0.35\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 86.98, Loss: 0.34\n",
      "  Valid Acc: 100.00, Loss: 0.06\n",
      "  Train Acc: 86.79, Loss: 0.35\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 86.75, Loss: 0.35\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 87.50, Loss: 0.32\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 87.54, Loss: 0.34\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 86.31, Loss: 0.35\n",
      "  Valid Acc: 99.29, Loss: 0.07\n",
      "  Train Acc: 86.51, Loss: 0.34\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 87.30, Loss: 0.34\n",
      "  Valid Acc: 99.64, Loss: 0.04\n",
      "  Train Acc: 85.67, Loss: 0.36\n",
      "  Valid Acc: 99.64, Loss: 0.05\n",
      "  Train Acc: 87.34, Loss: 0.34\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 86.15, Loss: 0.35\n",
      "  Valid Acc: 97.14, Loss: 0.10\n",
      "  Train Acc: 87.02, Loss: 0.34\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 87.06, Loss: 0.34\n",
      "  Valid Acc: 100.00, Loss: 0.03\n",
      "  Train Acc: 87.14, Loss: 0.32\n",
      "  Valid Acc: 99.64, Loss: 0.05\n",
      "  Train Acc: 87.22, Loss: 0.33\n",
      "  Valid Acc: 99.64, Loss: 0.06\n",
      "  Train Acc: 86.35, Loss: 0.35\n",
      "  Valid Acc: 99.64, Loss: 0.04\n",
      "  Train Acc: 85.95, Loss: 0.36\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 87.30, Loss: 0.33\n",
      "  Valid Acc: 100.00, Loss: 0.03\n",
      "  Train Acc: 86.07, Loss: 0.34\n",
      "  Valid Acc: 100.00, Loss: 0.04\n",
      "  Train Acc: 87.70, Loss: 0.33\n",
      "  Valid Acc: 99.64, Loss: 0.04\n",
      "  Train Acc: 86.79, Loss: 0.33\n",
      "  Valid Acc: 100.00, Loss: 0.03\n",
      "  Train Acc: 87.86, Loss: 0.33\n",
      "  Valid Acc: 100.00, Loss: 0.03\n",
      "  Train Acc: 86.55, Loss: 0.32\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 87.82, Loss: 0.32\n",
      "  Valid Acc: 100.00, Loss: 0.05\n",
      "  Train Acc: 88.49, Loss: 0.31\n",
      "  Valid Acc: 100.00, Loss: 0.03\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_65487) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_65487) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.9785714285714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 16:50:38.383916: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-06-16 16:50:38.383951: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-06-16 16:50:38.384467: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: model\n",
      "2024-06-16 16:50:38.387226: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-06-16 16:50:38.387249: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: model\n",
      "2024-06-16 16:50:38.391532: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n",
      "2024-06-16 16:50:38.393292: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-06-16 16:50:38.438268: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: model\n",
      "2024-06-16 16:50:38.455947: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 71482 microseconds.\n",
      "2024-06-16 16:50:38.530289: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 8, Total Ops 16, % non-converted = 50.00 %\n",
      " * 8 ARITH ops\n",
      "\n",
      "- arith.constant:    8 occurrences  (f32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully convert tflite float\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Importing a function (__inference_internal_grad_fn_65487) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n",
      "2024-06-16 16:50:39.180755: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-06-16 16:50:39.180788: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-06-16 16:50:39.181017: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: model\n",
      "2024-06-16 16:50:39.183615: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-06-16 16:50:39.183632: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: model\n",
      "2024-06-16 16:50:39.189431: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-06-16 16:50:39.231986: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: model\n",
      "2024-06-16 16:50:39.249317: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 68300 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 8, Total Ops 16, % non-converted = 50.00 %\n",
      " * 8 ARITH ops\n",
      "\n",
      "- arith.constant:    8 occurrences  (f32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 4)\n",
      "  (f32: 1)\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully convert tflite quant\n",
      "testing with model/float_model.tflite\n",
      "acc = 0.9785714285714285 (137 / 140)\n",
      "testing with model/quant_model.tflite\n",
      "acc = 0.9785714285714285 (137 / 140)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 1e-2\n",
    "epochs = 100\n",
    "\n",
    "dataset_path = \"../c20_e10_m15_data.csv\"\n",
    "# dataset_path = \"../c30_e5_dataset.csv\"\n",
    "output_class = 7\n",
    "save_dir = Path(\"./\")\n",
    "input_shape, train_dataset, valid_dataset, test_dataset = dataset_preproccessed(dataset_path, batch_size, [0.9, 0.10, 0.00], output_class)\n",
    "# input_shape, train_dataset, valid_dataset, test_dataset = pseudo_dataset_preproccessed(3000, 15, batch_size, [0.9, 0.05, 0.05], output_class)\n",
    "\n",
    "m = model([input_shape], output_class, learning_rate)\n",
    "m.build((batch_size , input_shape))\n",
    "m._model.summary()\n",
    "\n",
    "train_info = []\n",
    "max_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    epoch_info = train_one_epoch(m, train_dataset, valid_dataset, max_acc, save_dir)\n",
    "    max_acc = max(epoch_info[2], max_acc)\n",
    "    train_info.append(list(epoch_info))\n",
    "\n",
    "show_train_results(train_info , save_dir)\n",
    "testing_model(dataset_path, input_shape, output_class, save_dir, save_dir)\n",
    "save_model_tflite_quant(save_dir / \"model\", valid_dataset)\n",
    "\n",
    "# input_shape, train_dataset, valid_dataset, test_dataset = dataset_preproccessed(\"../c30_e5_dataset.csv\", 1, [0.3, 0.3, 0.4], output_class)\n",
    "test_tflite_model(save_dir / \"model\" / \"float_model.tflite\", dataset_path, input_shape, output_class)\n",
    "test_tflite_model(save_dir / \"model\" / \"quant_model.tflite\", dataset_path, input_shape, output_class)\n",
    "\n",
    "!xxd -i {save_dir}/model/quant_model.tflite > model/model.cc\n",
    "# !echo -ne \"#include \\\"model_data_quant.h\\\"\\nalignas(8)\\n\" > model/model_data_quant.h\n",
    "!cat model/model.cc > model/model_data_quant.h\n",
    "!sed -i -E 's/(unsigned\\s.*\\s).*(_len|\\[\\])/const \\1model\\2/g' model/model_data_quant.h\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 8\n",
    "# learning_rate = 1e-3\n",
    "# epochs = 50\n",
    "\n",
    "# dataset_path = \"../c30_e5_dataset.csv\"\n",
    "# output_class = 7\n",
    "# save_dir = Path(\"./\")\n",
    "# input_shape = 30 *6\n",
    "# test_tflite_model(save_dir / \"model\" / \"quant_model.tflite\", dataset_path, input_shape, output_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
