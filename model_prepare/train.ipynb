{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def expand_row_data(data, capture_point) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    label = []\n",
    "    expand_data = []\n",
    "\n",
    "    for idx in range(1, len(data), capture_point):\n",
    "        label.append(int(data[0]))\n",
    "        expand_data.append(np.array(data[idx : idx + capture_point]))\n",
    "\n",
    "    return label, expand_data\n",
    "\n",
    "def gen_tf_dataset(data, capture_point, batch_size, output_class):\n",
    "    label = []\n",
    "    expanded_data = []\n",
    "    for row in data:\n",
    "        l, e = expand_row_data(row, capture_point)    \n",
    "        \n",
    "        label += l\n",
    "        expanded_data += e\n",
    "    label = tf.one_hot(label, output_class)  \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((expanded_data, label))\n",
    "    dataset = dataset.shuffle(len(data), reshuffle_each_iteration=True)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def dataset_preproccessed(data_folder, batch_size, train_vaild_test_ratio, output_class):\n",
    "    data_folder = Path(data_folder)\n",
    "    capture_point, extra_point = data_folder.stem.split('_')[:-1]\n",
    "    capture_point = int(capture_point[1:])\n",
    "    extra_point = int(extra_point[1:])\n",
    "\n",
    "    \n",
    "    with open(data_folder, newline='') as data:\n",
    "        row_data = csv.reader(data, delimiter=',')\n",
    "        row_data = [i for i in row_data]\n",
    "\n",
    "    random.shuffle(row_data)\n",
    "    \n",
    "    train_data_cnt = int(len(row_data) * train_vaild_test_ratio[0])\n",
    "    valid_data_cnt = int(len(row_data) * train_vaild_test_ratio[1])\n",
    "    test_data_cnt = len(row_data) - train_data_cnt - valid_data_cnt\n",
    "    print(capture_point, extra_point)\n",
    "    print(train_data_cnt, valid_data_cnt, test_data_cnt)\n",
    "    \n",
    "    for idx, row in enumerate(row_data):\n",
    "        for c_idx, _ in enumerate(row): row[c_idx] = int(row[c_idx])\n",
    "\n",
    "    train_dataset = gen_tf_dataset(row_data[:train_data_cnt], capture_point, batch_size, output_class)\n",
    "    valid_dataset = gen_tf_dataset(row_data[train_data_cnt : train_data_cnt + valid_data_cnt], capture_point, batch_size, output_class)\n",
    "    test_dataset = gen_tf_dataset(row_data[train_data_cnt + valid_data_cnt : ], capture_point, batch_size, output_class)\n",
    "    \n",
    "    return capture_point, train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pseudo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pseudo_dataset(size, capture_point, batch_size, output_class):\n",
    "    label = []\n",
    "    expanded_data = []\n",
    "\n",
    "    x_values = np.random.uniform(low=0, high= output_class * 1000, size=size).astype(np.float32)\n",
    "\n",
    "    for x in x_values:\n",
    "        l = int(x // 1000) \n",
    "\n",
    "        e = [l for i in range(capture_point)]\n",
    "\n",
    "        label += [l]\n",
    "        expanded_data += [e]\n",
    "    label = tf.one_hot(label, output_class)  \n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((expanded_data, label))\n",
    "    dataset = dataset.shuffle(size, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def pseudo_dataset_preproccessed(total_data_size ,capture_point, batch_size, train_vaild_test_ratio, output_class):\n",
    "\n",
    "    \n",
    "    train_data_cnt = int(total_data_size * train_vaild_test_ratio[0])\n",
    "    valid_data_cnt = int(total_data_size * train_vaild_test_ratio[1])\n",
    "    test_data_cnt = total_data_size- train_data_cnt - valid_data_cnt\n",
    "    print(capture_point)\n",
    "    print(train_data_cnt, valid_data_cnt, test_data_cnt)\n",
    "\n",
    "    train_dataset = gen_pseudo_dataset(train_data_cnt, capture_point, batch_size, output_class)\n",
    "    valid_dataset = gen_pseudo_dataset(valid_data_cnt, capture_point, batch_size, output_class)\n",
    "    test_dataset = gen_pseudo_dataset(test_data_cnt, capture_point, batch_size, output_class)\n",
    "    \n",
    "    return capture_point, train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(tf.keras.Model):\n",
    "    def __init__(self , input_dim , out_class = 9 , learning_rate=1e-3):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.out_class = out_class\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self._model = self._build_model()\n",
    "        self._learner = self._build_learner()\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x:tf.Tensor, training:bool=False) -> tf.Tensor:\n",
    "        output = self._model(x, training=training)\n",
    "        return output\n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, x:tf.Tensor, y:tf.Tensor):\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self.__call__(x, training=True)\n",
    "            classLoss = self._learner[\"get_loss\"](y, output)\n",
    "            review = tf.math.in_top_k(tf.math.argmax(y,axis=1), output, 1)\n",
    "            perf = tf.math.reduce_mean(tf.cast(review, dtype=\"float32\"))\n",
    "\n",
    "        cGradients = tape.gradient(classLoss, self._model.trainable_variables)\n",
    "        self._learner[\"optimize\"].apply_gradients(zip(cGradients, self._model.trainable_variables))\n",
    "        return perf, classLoss\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def validate(self, x:tf.Tensor, y:tf.Tensor) -> tf.Tensor:\n",
    "        output = self.__call__(x, training=False)\n",
    "        classLoss = self._learner[\"get_loss\"](y , output)\n",
    "        review = tf.math.in_top_k(tf.math.argmax(y,axis=1), output, 1)\n",
    "        perf = tf.math.reduce_mean(tf.cast(review, dtype=\"float32\"))\n",
    "        return perf , classLoss\n",
    "\n",
    "    def _build_model(self) -> tf.keras.Model:\n",
    "\n",
    "        input_tensor = tf.keras.Input(shape=self.input_dim)\n",
    "        feature_map = input_tensor\n",
    "        \n",
    "        d = 12\n",
    "        feature_map = tf.keras.layers.Dense(d, input_dim = self.input_dim, activation='relu')(feature_map)\n",
    "        # feature_map = tf.keras.layers.Dropout(0.2)(feature_map)\n",
    "        # feature_map = tf.keras.layers.Dense(d, input_dim = d, activation='relu')(feature_map)\n",
    "        # feature_map = tf.keras.layers.Dropout(0.2)(feature_map)\n",
    "        # feature_map = tf.keras.layers.Dense(512, input_dim = 512, activation='relu')(feature_map)\n",
    "        output_tensor = tf.keras.layers.Dense(self.out_class, input_dim = d, activation=tf.keras.activations.softmax)(feature_map)\n",
    "        \n",
    "\n",
    "        model = tf.keras.Model(input_tensor, output_tensor)\n",
    "        return model\n",
    "    \n",
    "    def _build_learner(self) -> dict:\n",
    "        # classLoss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        classLoss = tf.keras.losses.CategoricalCrossentropy()\n",
    "        classOptimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        learner = {\"get_loss\": classLoss, \"optimize\": classOptimizer}\n",
    "\n",
    "        return learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model , train_dataloader , valid_dataloader , max_acc , root_dir : Path):\n",
    "\n",
    "    epoch_train_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_loss = []\n",
    "    epoch_valid_acc = []\n",
    "\n",
    "    for inData, outData in tqdm(train_dataloader):\n",
    "        acc , loss = model.train(inData, outData)\n",
    "        epoch_train_acc.append(acc)\n",
    "        epoch_train_loss.append(loss)\n",
    "        # print(loss)\n",
    "        # print(acc)\n",
    "    # raise ValueError\n",
    "        \n",
    "    for inData, outData in tqdm(valid_dataloader):\n",
    "        acc , loss = model.validate(inData, outData)\n",
    "        epoch_valid_acc.append(acc)\n",
    "        epoch_valid_loss.append(loss)\n",
    "\n",
    "    epoch_train_acc_mean = tf.math.reduce_mean(epoch_train_acc) * 100\n",
    "    epoch_valid_acc_mean = tf.math.reduce_mean(epoch_valid_acc) * 100\n",
    "    epoch_train_loss_mean = tf.math.reduce_mean(epoch_train_loss)\n",
    "    epoch_valid_loss_mean = tf.math.reduce_mean(epoch_valid_loss)\n",
    "\n",
    "    print(f\"  Train Acc: {epoch_train_acc_mean:.2f}, Loss: {epoch_train_loss_mean:.2f}\")\n",
    "    print(f\"  Valid Acc: {epoch_valid_acc_mean:.2f}, Loss: {epoch_valid_loss_mean:.2f}\")\n",
    "\n",
    "    if(epoch_valid_acc_mean > max_acc):\n",
    "        print(\"save best model\")\n",
    "        model.save(root_dir /\"model\", include_optimizer=False)\n",
    "    \n",
    "    return epoch_train_acc_mean, epoch_train_loss_mean , epoch_valid_acc_mean , epoch_valid_loss_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_model(test_dataloader , root_dir : Path = None , model_path : Path = None):\n",
    "    if root_dir is not None :\n",
    "        model = tf.keras.models.load_model(root_dir / \"model\", compile=False)\n",
    "    elif model_path is not None:\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "    else:\n",
    "        raise AttributeError(\"Testing models root_dir and model_path are not given\")\n",
    "    \n",
    "    testing_acc = []\n",
    "    print(\"Testing Model\")\n",
    "    \n",
    "    for data , label in tqdm(test_dataloader):\n",
    "        acc , loss = model.validate(data , label)\n",
    "        \n",
    "        testing_acc.append(acc)\n",
    "    \n",
    "    print(f\"acc = {np.mean(testing_acc)*100:.2f}\")\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_train_results(train_info , root_folder : Path):\n",
    "    train_info = np.array(train_info)\n",
    "\n",
    "    plt.plot(np.arange(1,train_info.shape[0]+1) , train_info[:,0] , 'r' , label='train')\n",
    "    plt.plot(np.arange(1,train_info.shape[0]+1) , train_info[:,2] , 'b' , label='valid')\n",
    "    plt.title(\"Acc\")\n",
    "    plt.savefig(root_folder / \"model\" / \"Acc.png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.arange(1,train_info.shape[0]+1) , train_info[:,1] , 'r' , label='train')\n",
    "    plt.plot(np.arange(1,train_info.shape[0]+1) , train_info[:,3] , 'b' , label='valid')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.savefig(root_folder / \"model\" / \"Loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_tflite_quant(save_model_folder : Path , dataset):\n",
    "\n",
    "    def representative_dataset():\n",
    "        idx = 0\n",
    "        for data , label in dataset:\n",
    "            # if idx > 1000:\n",
    "            #     break\n",
    "            # idx += 1\n",
    "            \n",
    "            yield [data]\n",
    "\n",
    "    # float\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(str(save_model_folder))\n",
    "    tflite_model = converter.convert()\n",
    "    open(save_model_folder / 'float_model.tflite', 'wb').write(tflite_model)\n",
    "    print(\"Successfully convert tflite float\")\n",
    "\n",
    "    # quant\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(str(save_model_folder))\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "    converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "    tflite_quant_model = converter.convert()\n",
    "    open(save_model_folder / 'quant_model.tflite', 'wb').write(tflite_quant_model)\n",
    "    print(\"Successfully convert tflite quant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tflite_model(tflite_model_path, test_dataset):\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "    input = interpreter.get_input_details()[0]\n",
    "    output = interpreter.get_output_details()[0]\n",
    "    input_scale, input_zero_point = input['quantization']\n",
    "    output_scale, output_zero_point = output[\"quantization\"]\n",
    "    \n",
    "    print(f\"testing with {tflite_model_path}\")\n",
    "    \n",
    "    testing_acc = np.array([])\n",
    "    for data , label in tqdm(test_dataset):\n",
    "        if input[\"dtype\"] == np.float32:\n",
    "            interpreter.set_tensor(input['index'], data)\n",
    "        else:\n",
    "            quant_data = data.numpy() / input_scale + input_zero_point\n",
    "            interpreter.set_tensor(input['index'], quant_data.astype(input[\"dtype\"] ))\n",
    "\n",
    "        interpreter.invoke()\n",
    "\n",
    "        if output[\"dtype\"] == np.float32:\n",
    "            pred = interpreter.get_tensor(output['index'])\n",
    "        else:\n",
    "            dequant_pred = interpreter.get_tensor(output['index']).astype(np.float32)\n",
    "            pred = (dequant_pred - output_zero_point) * output_scale\n",
    "\n",
    "        testing_acc = np.append(testing_acc, np.argmax(pred) == np.argmax(label))   \n",
    "    \n",
    "    print(f\"result acc = {np.mean(testing_acc)*100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "output_class = 10\n",
    "save_dir = Path(\"./\")\n",
    "# input_shape, train_dataset, valid_dataset, test_dataset = dataset_preproccessed(\"../c3_e3_dataset.csv\", batch_size, [0.9, 0.05, 0.05], output_class)\n",
    "input_shape, train_dataset, valid_dataset, test_dataset = pseudo_dataset_preproccessed(3000, 15, batch_size, [0.9, 0.05, 0.05], output_class)\n",
    "\n",
    "m = model([input_shape], output_class, learning_rate)\n",
    "m.build((batch_size , input_shape))\n",
    "m._model.summary()\n",
    "\n",
    "train_info = []\n",
    "max_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    epoch_info = train_one_epoch(m, train_dataset, valid_dataset, max_acc, save_dir)\n",
    "    max_acc = max(epoch_info[2], max_acc)\n",
    "    train_info.append(list(epoch_info))\n",
    "\n",
    "show_train_results(train_info , save_dir)\n",
    "testing_model(test_dataset, save_dir, save_dir)\n",
    "save_model_tflite_quant(save_dir / \"model\", valid_dataset)\n",
    "\n",
    "input_shape, train_dataset, valid_dataset, test_dataset = pseudo_dataset_preproccessed(1000, 15, 1, [0.01, 0.01, 0.98], output_class)\n",
    "test_tflite_model(save_dir / \"model\" / \"float_model.tflite\", test_dataset)\n",
    "test_tflite_model(save_dir / \"model\" / \"quant_model.tflite\", test_dataset)\n",
    "\n",
    "!xxd -i {save_dir}/model/quant_model.tflite > model/model.cc\n",
    "# !echo -ne \"#include \\\"model_data_quant.h\\\"\\nalignas(8)\\n\" > model/model_data_quant.h\n",
    "!cat model/model.cc > model/model_data_quant.h\n",
    "!sed -i -E 's/(unsigned\\s.*\\s).*(_len|\\[\\])/const \\1model\\2/g' model/model_data_quant.h\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
